<!DOCTYPE html>
<!-- saved from url=(0064)https://blackrockdigital.github.io/startbootstrap-scrolling-nav/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Collision Sound Source Segmentation</title>

    <!-- Bootstrap core CSS -->
    <link href="./assets/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="./assets/scrolling-nav.css" rel="stylesheet">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/wavesurfer.js/1.2.3/wavesurfer.min.js"></script>

    <script>

        $(document).ready(function(){
            var default_h = 384;
            var default_w = 512;

            function initWaveSurfer(name, domEl, url) {
              window[name] = WaveSurfer.create({
                container: domEl,
                waveColor: 'blue',
                progressColor: 'green'
              });
              window[name].load(url);
            }

            var sound_srcs = $('*[data-src*=sound-of-pixels]:visible');
            for (var i = 0, len = sound_srcs.length; i < len; i++){
                var e = sound_srcs[i];
                container_name = '#' + e.getAttribute("id");
                url = e.getAttribute("data-src");
                if (container_name.includes('input')) {
                    index = container_name.split('input')[1];
                    wavesurfer_name = 'soundtrack' + index;
                } else {
                    index = container_name.split('demo')[1];
                    wavesurfer_name = 'wavesurfer' + index;
                }
                initWaveSurfer(wavesurfer_name, container_name, url )
            }


            $("video").click(function (e) {
                var video_name = $(this).attr('class').split(' ')[0];
                var video = $("video." + video_name)[0];
                if (video_name.includes("input")) {
                    playOriginal(video_name, video)
                } else {
                    playDemo(video_name, video, e)
                }
            });

            function playOriginal(video_name, video) {
              wave_surfer_name = 'soundtrack' + video_name.split('Video')[1];
              wavesurfer = window[wave_surfer_name];
              video.muted = true;
              video.currentTime = 0;
              video.play();
              wavesurfer.play([0]);
            }

            function playDemo(video_name, video, e) {
              index = video_name.split('Video')[1];
              wave_surfer_name = 'wavesurfer' + index;
              wavesurfer = window[wave_surfer_name];
              var scale = 16;
              var offset = $(video).offset();
              var xPos = e.pageX - offset.left;
              var yPos = e.pageY - offset.top;
              var x = Math.floor(xPos/video.clientWidth*default_w/scale)+1;
              var y = Math.floor(yPos/video.clientHeight*default_h/scale)+1;
              var x = Math.min(Math.max(x, 1), 32);
              var y = Math.min(Math.max(y, 1), 24);
              // console.log(x, y);
              var sound_src = $(video)[0].src.slice(0, -4) + '/' + x + '_' + y + '.wav';
              video.muted = true;
              video.currentTime = 0;

              // wavesurfer:
              wavesurfer.empty();
              wavesurfer.load(sound_src);
              wavesurfer.on('ready', function () {
                  video.play();
                  wavesurfer.play();
              });

              // top is vertical direction, left is horizontal position
              // offset chosen by inspection
              var newtop = String((y-1) * video.clientHeight / default_h * scale + 40) +'px';
              var newleft = String((x-1) * video.clientWidth / default_w * scale + 15) + 'px';
              // console.log(newtop, newleft);
              $(".overlay#dot" + index).css({"opacity": "0.5", "background": "red", "top": newtop, "left": newleft});
            }

        });

    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-60734989-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-60734989-3');
    </script>

    <style>
      p.lead a {
        color: rgb(10, 96, 155);
      }

      h1 {
        font-size: 3.5rem;
      }
      h2 {
        text-align: center;
      }
      video {
        cursor: pointer;
      }

      .overlay {
          height: 17.625px;
          width: 22.727px;
          position: absolute;
          top: 45px;
          left: 15px;
          z-index: 2;
          background: red;
          opacity: 0;
          border-radius: 80px
        }

      header.bg-primary {
        margin-bottom: -2rem;
        height: auto;
        padding-bottom: 0;
      }

      .highlight pre {
        font-size: 16px !important;
      }
      
      #reference p {
        font-size: 18px;
      }
    </style>

    

    <!-- MathJax for LaTeX equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        chtml: {
          fontURL: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2',
          matchFontHeight: false
        },
        options: {
          enableMenu: false
        }
      };
    </script>

    <!-- Add custom CSS to control MathJax font weight -->
    <style>
      .MathJax, .MathJax_Display, .MathJax span {
        font-weight: normal !important;
        font-family: inherit !important;
      }
      
      mjx-container mjx-math {
        font-weight: normal !important;
      }
      
      mjx-container {
        font-weight: normal !important;
      }
    </style>

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Collision Sound Source Segmentation</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#abstract">Abstract</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#approach">Approach Overview</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#download">Download</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#res">Results</a>
            </li>
            <!-- <li class="nav-item">
                <a class="nav-link js-scroll-trigger" href="#sup-res">Supplementary Results</a>
            </li> -->
          </ul>
        </div>
      </div>
    </nav>

    <header class="bg-primary text-black">
      <div class="container text-center" style="margin-top:-5rem; padding-bottom: 1rem;">
        <h2>Segmenting Collision Sound Sources in Egocentric Videos</h2>
        <p class="lead">
          <font size="5"><a href="https://krantiparida.github.io/">Kranti Kumar Parida</a><sup> 1</sup>,
          <a href="https://omar-emara.github.io/">Omar Emara</a><sup>1</sup>,
          <a href="https://hazeldoughty.github.io/">Hazel Doughty</a><sup>2</sup>,
          <a href="https://dimadamen.github.io/">Dima Damen</a><sup>1</sup>
          <p><sup>1</sup>University of Bristol, <sup>2</sup>Leiden University</p>
          </font>  
      </div>
    </header>

  
    <section id="abstract" style="margin-top:-8rem;margin-bottom:-15rem;">
      <div class="container"> <h2>Abstract</h2>
        <div class="row">
          <div class="col-lg-12 text-center"><img src="./cs3/img/teaser.jpg" width="50%"></div>
        </div>
        <div class="row">
          <div class="col-lg-12 mx-auto">
            <p class="lead" align="justify">
              Humans excel at multisensory perception and can often recognise object properties from the sound of their interactions. Inspired by this, we propose the novel task of Collision Sound Source Segmentation (CS3), where we aim to segment the objects responsible for a collision sound in visual input (i.e. video frames from the collision clip), conditioned on the audio. This task presents unique challenges. Unlike isolated sound events, a collision sound arises from interactions between two objects, and the acoustic signature of the collision depends on both. We focus on egocentric video, where sounds are often clear, but the visual scene is cluttered, objects are small, and interactions are brief.
              To address these challenges, we propose a weakly-supervised method for audio-conditioned segmentation, utilising foundation models (CLIP and SAM2).
              We also incorporate egocentric cues, i.e. objects in hands, to find acting objects that can potentially be collision sound sources. Our approach outperforms competitive baselines by $3 \times$ and $4.7 \times$ in mIoU on two benchmarks we introduce for the CS3 task: EPIC-CS3 and Ego4D-CS3.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section id="approach" style="margin-top:-5rem;margin-bottom:-9rem;">
      <div class="container"> <h2>Approach Overview</h2>
        <div class="row">
          <div class="col-lg-12 text-center"><img src="./cs3/img/approach.jpg" width="100%"></div>
        </div>
        <div class="row">
          <div class="col-lg-12 mx-auto"> 
            <p class="lead" align="justify">
              We define our task, Collision Sound Source Segmentation (CS3) as follows: given an audio-visual clip containing a single collision sound, we aim to identify the object(s) causing this collision sound, and output pixel-level segmentations of these objects in a frame within the collision segment. 
              We define a collision sound as one produced when two or more objects (or parts of one object) exert force on each other as a result of an interaction. Crucially, such sound results from the interaction itself and cannot be attributed to either object alone. 
              Our approach is trained with weak supervision, i.e. we require only temporally segmented clips of collisions, without any labels of colliding objects, action semantics or object segmentation masks. 

              Formally, given a collision audio $\mathbf{A}{\in}\mathbb{R}^{1 \times T}$ and an image $\mathbf{I}{\in}\mathbb{R}^{3 \times H \times W}$ from a video, we learn a function to segment object(s) responsible for the collision:
              $$f: (\mathbf{I}, \mathbf{A}) \rightarrow \{\mathbf{M}_k\}_{k=1}^n$$ where $n{\in}\{1, 2\}$, and each $\mathbf{M}_k{\in}\{0, 1\}^{H{\times}W}$ denotes the mask for the $k^\text{th}$ colliding object.
            </p>
            <p class="lead" align="justify">
              Given our videos are egocentric, we use the interaction prior that sounds are often caused by the camera wearer manipulating one of the objects involved in the collision.
              Guided by this hypothesis, we combine two complementary cues: (1) audio-visual correlation to localise the sound-producing object, and (2) hand-object interaction priors to identify objects held in both hands. 
              <!-- Finally, we refine the masks and verify the colliding object(s) to obtain the corresponding segmentation masks. -->
              Our architecture consists of three main components: (1) audio-conditioned segmentation, (2) hand-object interaction (HOI) and (3) collision verification. 
              The audio-conditioned segmentation model takes an image ($\mathbf{I}$) and its corresponding audio ($\mathbf{A}$) to produce conditioning signals $\mathbf{I}_C$ and $\mathbf{A}_C$. The audio is first encoded into a representation aligned with the text token space, which is used alongside visual features to guide the localisation of sound-producing regions. 
              The model is trained with image-level ($\mathcal{L}_{i}$), feature-level ($\mathcal{L}_{f}$), area regaularisation ($\mathcal{L}_{r}$) losses. 
              The HOI model provides bounding boxes for in-hand left and right objects when present. The collision verification module uses SAM to extract object masks for audio-conditioned segmentation mask $\mathbf{M}_{av}$ and in-hand objects $\mathbf{M}_{\textit{left}}$ and $\mathbf{M}_{\textit{right}}$. 
              A contact-based strategy is then applied to estimate the segmentations for collision sound sources, $\mathbf{M}_{coll.}$.

            </p>
          </div>
        </div>
      </div>
    </section>

    <section id="download" class="bg-light">
      <div class="container" style="margin-top:-5rem;margin-bottom:-5rem;">
        <div class="row">
          <div class="col-lg-4 mx-auto text-center">
            <h2>Paper</h2>
            <a target="_blank " href=""><img src="./b_img_depth/img/paper.png" height=200 style="border:1px solid black;"></a>
          </div>
          <div class="col-lg-4 mx-auto text-center">
            <h2>Supplementary</h2>
            <a target="_blank " href=""><img src="./b_img_depth/img/supple.png" height=200 style="border:1px solid black;"></a>
          </div>
          <div class="col-lg-4 mx-auto text-center">
            <h2>Code, Model, Dataset</h2>
            <a target="_blank " href=""><img src="./b_img_depth/img/git-logo.png" height=200 style="border:1px solid black;"></a>
          </div>
    
        </div>
      </div>
    </section>

    <section id="res" style="margin-bottom:-10rem;width:100%;">
        <div class="container" style="margin-top:-5rem;width:100%;">
        <div class="row">
            <div class="col-lg-12 mx-auto"><h2>Video Results</h2></div>
        </div>
        <div class="row" style="margin-top:10px;">   
            <div class="col-lg-12 mx-auto" >
                <div>
                    <video width="100%" controls>
                        <source src="./cs3/img/supp_35.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        </div>
    </section>            
    
    <section id="img-res" style="margin-bottom:-5rem;width:100%;">
      <div class="container" style="margin-top:-5rem;width:100%;">
        <!-- <div class="row" style="margin-top:10px;">
          <div class="col-lg-12 mx-auto"><h2>Qualitative Results</h2></div>
        </div> -->
        <div class="row" style="margin-top:10px;">
          <div class="col-lg-6 mx-auto text-center"><h2>Qualitative Results</h2>
            <img src="./cs3/img/qual_res.jpg" width="98%">
          </div>
          <div class="col-lg-6 mx-auto text-center"><h2>Failure Cases</h2>
            <img src="./cs3/img/failure_cases.jpg" width="98%">
          </div>
        </div>
      </div>
    </section>

    <section id="reference" class="bg-light">
      <div class="container" style="margin-top:-8rem;margin-bottom:-15rem;width:100%;">
        <div class="row" style="margin-top:10px;">
          <div class="col-lg-12 mx-auto"><h2>Bibtex</h2></div>
        </div>
        <div class="row">
          <div class="col-lg-12 mx-auto">
            <p>If you use the code or dataset from the project, please cite:</p>
            <div class="highlight highlight-text-bibtex">
              <pre>
                <span class="pl-k">@InProceedings</span>{<span class="pl-en">parida2025segmenting</span>,
                <span class="pl-s">author</span> = <span class="pl-s"><span class="pl-pds">{</span>Parida, Kranti and Emara, Omar and Doughty, Hazel and Damen, Dima<span class="pl-pds">}</span></span>,
                <span class="pl-s">title</span> = <span class="pl-s"><span class="pl-pds">{</span>Segmenting Collision Sound Sources in Egocentric Videos<span class="pl-pds">}</span></span>,
                <span class="pl-s">booktitle</span> = <span class="pl-s"><span class="pl-pds">{</span><span class="pl-pds">}</span></span>,
                <span class="pl-s">year</span> = <span class="pl-s"><span class="pl-pds">{</span>2025<span class="pl-pds">}</span></span>}
              </pre>
            </div>
          </div>
        </div>  
    </section>

    <section id="acknowledgements" class="bg-light">
      <div class="container" style="margin-top:-10rem;margin-bottom:-10rem;width:100%;">
        <div class="row" style="margin-top:10px;">
          <div class="col-lg-12 mx-auto"><h2>Acknowledgements</h2></div>
        </div>
        <div class="row">
          <div class="col-lg-12 mx-auto">
            <p></p>
          </div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer class="py-5 bg-dark">
      <div class="container">
        <p class="m-0 text-center text-white">Webpage Template Courtesy <a href="http://sound-of-pixels.csail.mit.edu/">CSAIL, MIT</a>.</p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="./assets/jquery.min.js"></script>
    <script src="./assets/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./assets/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="./assets/scrolling-nav.js"></script>


</body></html>
